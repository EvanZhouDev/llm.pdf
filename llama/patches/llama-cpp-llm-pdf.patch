diff --git a/CMakeLists.txt b/CMakeLists.txt
index 1acf4bb08..da0e9018f 100644
--- a/CMakeLists.txt
+++ b/CMakeLists.txt
@@ -1402,6 +1402,7 @@ install(FILES "${CMAKE_CURRENT_BINARY_DIR}/llama.pc"
 #
 
 add_subdirectory(common)
+add_subdirectory(examples/main)
 
 if (LLAMA_BUILD_TESTS AND NOT CMAKE_JS_VERSION)
     include(CTest)
diff --git a/examples/main/main.cpp b/examples/main/main.cpp
index cfaf6a6e8..52a51f2e0 100644
--- a/examples/main/main.cpp
+++ b/examples/main/main.cpp
@@ -14,6 +14,7 @@
 #include <sstream>
 #include <string>
 #include <vector>
+#include <emscripten.h>
 
 #if defined (__unix__) || (defined (__APPLE__) && defined (__MACH__))
 #include <signal.h>
@@ -126,6 +127,10 @@ static std::string chat_add_and_format(struct llama_model * model, std::vector<l
 }
 
 int main(int argc, char ** argv) {
+    EM_ASM({
+        write_file(file_name, file_data);
+    });
+    
     gpt_params params;
     g_params = &params;
 
@@ -931,6 +936,9 @@ int main(int argc, char ** argv) {
         llama_state_save_file(ctx, path_session.c_str(), session_tokens.data(), session_tokens.size());
     }
 
+    printf("\n");
+    fflush(stdout);
+
     llama_print_timings(ctx);
     write_logfile(ctx, params, model, input_tokens, output_ss.str(), output_tokens);
 
diff --git a/ggml-backend.c b/ggml-backend.c
index 13c71c310..8b9ef7d77 100644
--- a/ggml-backend.c
+++ b/ggml-backend.c
@@ -908,7 +908,7 @@ void ggml_backend_cpu_set_abort_callback(ggml_backend_t backend_cpu, ggml_abort_
 }
 
 GGML_CALL ggml_backend_buffer_t ggml_backend_cpu_buffer_from_ptr(void * ptr, size_t size) {
-    GGML_ASSERT((uintptr_t)ptr % TENSOR_ALIGNMENT == 0 && "buffer pointer must be aligned");
+    // GGML_ASSERT((uintptr_t)ptr % TENSOR_ALIGNMENT == 0 && "buffer pointer must be aligned");
     return ggml_backend_buffer_init(ggml_backend_cpu_buffer_type(), cpu_backend_buffer_i_from_ptr, ptr, size);
 }
 
@@ -2027,9 +2027,9 @@ void ggml_backend_tensor_alloc(ggml_backend_buffer_t buffer, struct ggml_tensor
     GGML_ASSERT(tensor->buffer == NULL);
     GGML_ASSERT(tensor->data == NULL);
     GGML_ASSERT(tensor->view_src == NULL);
-    GGML_ASSERT(addr >= ggml_backend_buffer_get_base(buffer));
-    GGML_ASSERT((char *)addr + ggml_backend_buffer_get_alloc_size(buffer, tensor) <=
-                (char *)ggml_backend_buffer_get_base(buffer) + ggml_backend_buffer_get_size(buffer));
+    // GGML_ASSERT(addr >= ggml_backend_buffer_get_base(buffer));
+    // GGML_ASSERT((char *)addr + ggml_backend_buffer_get_alloc_size(buffer, tensor) <=
+    //             (char *)ggml_backend_buffer_get_base(buffer) + ggml_backend_buffer_get_size(buffer));
 
     tensor->buffer = buffer;
     tensor->data = addr;
